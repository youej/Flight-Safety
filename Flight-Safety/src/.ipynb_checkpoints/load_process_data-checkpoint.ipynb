{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e8e9c1-a135-480d-9365-637eb238ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85cf23d5-f4b9-4576-869b-d8d7679eacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_list = ['LATP', 'LONP','RALT','GS','TAS','IVV','BLAC','CTAC','FPAC','LATG','N1_1','FLAP','PTCH','ROLL','AOA1',\\\n",
    "            'AIL_1','ELEV_1','RUDD','LOC','GLS','ALT','LATG','PTRM','LONG','OIT_1']\n",
    "\n",
    "def load_data(read_url):\n",
    "    print('Loading data ...')\n",
    "    train_y_list = np.load(read_url + 'train_y_list.npy', allow_pickle=True)\n",
    "    test_y_list = np.load(read_url + 'test_y_list.npy', allow_pickle=True)\n",
    "    train_X_list = np.load(read_url + 'train_x_list.npy', allow_pickle=True)\n",
    "    test_X_list = np.load(read_url + 'test_x_list.npy', allow_pickle=True)\n",
    "\n",
    "    print(\"Data shapes: \")\n",
    "    print(f\"train_x_list: {train_X_list.shape}\")\n",
    "    print(f\"training data within each fold: {train_X_list[0].shape}\")\n",
    "    print(f\"train_y_list: {train_y_list.shape}\")\n",
    "    print(f\"training label within each fold: {train_y_list[0].shape}\")\n",
    "    \n",
    "    return train_y_list, test_y_list, train_X_list, test_X_list\n",
    "\n",
    "def reshape_data(data):\n",
    "    # Ensure data is a 3D array: (samples, timesteps, features)\n",
    "    num_samples, num_timesteps, num_features = data.shape\n",
    "    \n",
    "    # Flatten the data to 2D for standardization\n",
    "    flattened_data = data.reshape(num_samples * num_timesteps, num_features)\n",
    "    return flattened_data, (num_samples, num_timesteps, num_features)\n",
    "\n",
    "def unflatten_data(flat_data, original_shape):\n",
    "    num_samples, num_timesteps, num_features = original_shape\n",
    "    return flat_data.reshape(num_samples, num_timesteps, num_features)\n",
    "\n",
    "def preprocess_data(train_X_list, test_X_list):\n",
    "    print('Preprocessing data ...')\n",
    "    # Find indices of 'TAS' and 'GS'\n",
    "    excluded_tags = ['TAS', 'GS']\n",
    "    excluded_index = [para_list.index(tag) for tag in excluded_tags if tag in para_list]\n",
    "    \n",
    "    # Calculate indices to keep\n",
    "    keep_index = np.sort(list(set(np.arange(train_X_list[0].shape[-1])) - set(excluded_index)))\n",
    "\n",
    "    # Filter the data\n",
    "    train_x_list_filtered = [data[:, :, keep_index] for data in train_X_list]\n",
    "    test_x_list_filtered = [data[:, :, keep_index] for data in test_X_list]\n",
    "    \n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Standardize each 3D array in the list\n",
    "    for i in range(len(train_X_list)):  # Adjust this range based on the actual length of your lists\n",
    "        # Reshape train and test data\n",
    "        flattened_train_data, train_shape = reshape_data(train_x_list_filtered[i])\n",
    "        flattened_test_data, test_shape = reshape_data(test_x_list_filtered[i])\n",
    "\n",
    "        # Fit scaler on train data and transform both train and test data\n",
    "        scaler.fit(flattened_train_data)\n",
    "        train_standardized = scaler.transform(flattened_train_data)\n",
    "        test_standardized = scaler.transform(flattened_test_data)\n",
    "\n",
    "        # Unflatten data back to 3D\n",
    "        train_x_list_filtered[i] = unflatten_data(train_standardized, train_shape)\n",
    "        test_x_list_filtered[i] = unflatten_data(test_standardized, test_shape)\n",
    "\n",
    "    return train_x_list_filtered, test_x_list_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6b6b85-939e-4cdc-a9f5-36bef2a1db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pad_data(data, percentage_map, original_length):\n",
    "    \"\"\"\n",
    "    Truncate the dataset based on specified percentages and their proportions, then pad back to original length.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.array): The original dataset.\n",
    "    - percentage_map (dict): A dictionary where keys are tuples representing the percentage intervals of the original length to keep,\n",
    "                             and values are the proportion of the dataset to be truncated to this length.\n",
    "    - original_length (int): The original length of the sequences.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: The truncated and padded dataset.\n",
    "    \"\"\"\n",
    "    np.random.shuffle(data)  # Shuffle the data to randomize the order\n",
    "    total_samples = len(data)\n",
    "    truncated_padded_datasets = []\n",
    "\n",
    "    start_index = 0\n",
    "    for interval, proportion in percentage_map.items():\n",
    "        min_percentage, max_percentage = interval  # Unpack the interval tuple\n",
    "        num_samples = int(total_samples * proportion)\n",
    "        end_index = start_index + num_samples\n",
    "        \n",
    "        # Slice the data segment\n",
    "        data_segment = data[start_index:end_index]\n",
    "        \n",
    "        # Truncate each sequence in the segment\n",
    "        truncated_segment = []\n",
    "        for sequence in data_segment:\n",
    "            if min_percentage == max_percentage:\n",
    "                print(\"HELLO\")\n",
    "                random_percentage = min_percentage\n",
    "            else:\n",
    "                random_percentage = np.random.randint(min_percentage, max_percentage)\n",
    "            truncated_length = int(original_length * (random_percentage / 100))\n",
    "            truncated_sequence = sequence[:truncated_length]\n",
    "            padded_sequence = np.pad(truncated_sequence, ((0, original_length - len(truncated_sequence)), (0, 0)), mode='constant')\n",
    "            truncated_segment.append(padded_sequence)\n",
    "        \n",
    "        truncated_segment = np.array(truncated_segment)\n",
    "        truncated_padded_datasets.append(truncated_segment)\n",
    "        \n",
    "        start_index = end_index  # Update start index for the next segment\n",
    "\n",
    "    # Concatenate all truncated and padded segments\n",
    "    final_truncated_padded_data = np.concatenate(truncated_padded_datasets, axis=0)\n",
    "    \n",
    "    return final_truncated_padded_data\n",
    "    \n",
    "def shuffle_data(train_x_list, train_y_list):\n",
    "    \"\"\"\n",
    "    Shuffle the training data and labels in the same order.\n",
    "\n",
    "    Parameters:\n",
    "    - train_x_list (np.array): The dataset of input features.\n",
    "    - train_y_list (np.array): The dataset of labels.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: The shuffled dataset of input features.\n",
    "    - np.array: The shuffled dataset of labels.\n",
    "    \"\"\"\n",
    "    # Generate random indices for shuffling\n",
    "    indices = np.arange(len(train_x_list))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Apply the shuffled indices to both train_x_list and train_y_list\n",
    "    shuffled_x_list = train_x_list[indices]\n",
    "    shuffled_y_list = train_y_list[indices]\n",
    "\n",
    "    return shuffled_x_list, shuffled_y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f6a14-353e-4f2d-995b-c235f41eb206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
